{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-08T02:47:21.197990500Z",
     "start_time": "2023-10-08T02:47:00.169688400Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset.dataset import get_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "from final.matcher import OIE_Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "matcher = OIE_Match()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:06:23.849715400Z",
     "start_time": "2023-10-04T00:06:20.573210700Z"
    }
   },
   "id": "f4a85de5af59e3e7"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import json\n",
    "from OIE.utils.contractions import transform_portuguese_contractions\n",
    "\n",
    "\n",
    "def load_bio_datasets(path:str, length:int=None):\n",
    "    pt = f'datasets/validated_splits/normal/TA4/{path}'\n",
    "    dataset_pt = dict()\n",
    "    with open(pt, \"r\", encoding=\"utf-8\") as f_pt:\n",
    "        dataset = {}\n",
    "        actual_pos = 0\n",
    "        raw_txt = f_pt.read()\n",
    "        sentences = raw_txt.split(\"\\n\\n\")\n",
    "        sentences = [sent.split(\"\\n\") for sent in sentences]\n",
    "        sentences = [[s.split(\"\\t\") for s in sent] for sent in sentences]\n",
    "        if length is None:\n",
    "            stop = len(sentences)\n",
    "        else:\n",
    "            stop = length\n",
    "        i = 0\n",
    "        for sent in sentences:\n",
    "            if len(sent)>=3:\n",
    "                full_sent = []\n",
    "                arg0 = []\n",
    "                rel = []\n",
    "                arg1 = []\n",
    "                for tk in sent:\n",
    "                    if len(tk) == 10:\n",
    "                        full_sent.append(tk[0])\n",
    "                        if 'ARG0' in tk[-2]:\n",
    "                            arg0.append(tk[0])\n",
    "                        elif 'V' in tk[-2]:\n",
    "                            rel.append(tk[0])\n",
    "                        elif 'ARG1' in tk[-2]:\n",
    "                            arg1.append(tk[0])\n",
    "                full_sent = ' '.join(full_sent)\n",
    "                arg0 = ' '.join(arg0)\n",
    "                rel = ' '.join(rel)\n",
    "                arg1 = ' '.join(arg1)\n",
    "                dataset[i] = {\n",
    "                    'sent': full_sent,\n",
    "                    'arg1': arg0,\n",
    "                    'rel': rel,\n",
    "                    'arg2': arg1\n",
    "                }\n",
    "                i += 1\n",
    "\n",
    "        for sentence in dataset:\n",
    "            dataset_pt[actual_pos] = {\"phrase\": dataset[sentence]['sent'], \"extractions\": []}\n",
    "            arg1 = dataset[sentence][\"arg1\"]\n",
    "            arg2 = dataset[sentence][\"arg2\"]\n",
    "            rel = dataset[sentence][\"rel\"]\n",
    "\n",
    "            dataset_pt[actual_pos][\"extractions\"].append(\n",
    "                {\n",
    "                    \"arg1\": transform_portuguese_contractions(arg1),\n",
    "                    \"rel\": transform_portuguese_contractions(rel),\n",
    "                    \"arg2\": transform_portuguese_contractions(arg2),\n",
    "                    \"valid\": 1,\n",
    "                }\n",
    "            )\n",
    "            actual_pos += 1\n",
    "            if actual_pos >= stop:\n",
    "                break\n",
    "        return dataset_pt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T03:00:28.883634Z",
     "start_time": "2023-10-08T03:00:28.819705700Z"
    }
   },
   "id": "aa2c742b9658eb33"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "ds = load_bio_datasets('s2_alan_valid_corpus.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T03:04:01.076276500Z",
     "start_time": "2023-10-08T03:04:00.204911100Z"
    }
   },
   "id": "9f8788af4c7c42cb"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "936"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T03:04:02.472251900Z",
     "start_time": "2023-10-08T03:04:02.447095300Z"
    }
   },
   "id": "fed9f26eaa96a83a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON\n"
     ]
    }
   ],
   "source": [
    "text = 'ele'\n",
    "doc = nlp(text)\n",
    "for tk in doc:\n",
    "    print(tk.pos_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T23:46:26.029551800Z",
     "start_time": "2023-10-03T23:46:25.986561100Z"
    }
   },
   "id": "9d1ab36ee5ea10d5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def save_data(dataset, name: str):\n",
    "    dir = 'datasets/outputs/clean_dataset'\n",
    "    path = pathlib.Path(dir)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    file_text = \"\"\n",
    "    before_tag = '\\tXX\\t-\\t-\\t-\\t-\\t-\\t*\\t'\n",
    "    for key in tqdm(dataset, desc='saving data'):\n",
    "        ext = dataset[key]['ext']['splited']\n",
    "        sent = dataset[key]['sent']['sent']\n",
    "        match = matcher.match(sent=sent, \n",
    "                              arg1=ext['arg0'], \n",
    "                              rel=ext['rel'],\n",
    "                              arg2=ext['arg1'])\n",
    "        if match[-1]:\n",
    "            for i,token in enumerate([token.text for token in nlp(sent)]):\n",
    "                if i in range(match[0][0], match[0][1]+1):\n",
    "                    if len(range(match[0][0], match[0][1]+1)) == 1:\n",
    "                        file_text += token + before_tag + \"S-ARG0\" + \"\\n\"\n",
    "                    elif i == match[0][0]:\n",
    "                        file_text += token + before_tag + \"B-ARG0\" + \"\\n\"\n",
    "                    elif i > match[0][0] and i<match[0][1]:\n",
    "                        file_text += token + before_tag + \"I-ARG0\" + \"\\n\"\n",
    "                    elif i == match[0][1]:\n",
    "                        file_text += token + before_tag + \"E-ARG0\" + \"\\n\"\n",
    "\n",
    "                elif i in range(match[1][0], match[1][1]+1):\n",
    "                    if len(range(match[1][0], match[1][1] + 1)) == 1:\n",
    "                        file_text += token + before_tag + \"S-V\" + \"\\n\"\n",
    "                    elif i == match[1][0]:\n",
    "                        file_text += token + before_tag + \"B-V\" + \"\\n\"\n",
    "                    elif i > match[1][0] and i < match[1][1]:\n",
    "                        file_text += token + before_tag + \"I-V\" + \"\\n\"\n",
    "                    elif i == match[1][1]:\n",
    "                        file_text += token + before_tag + \"E-V\" + \"\\n\"\n",
    "\n",
    "                elif i in range(match[2][0], match[2][1]+1):\n",
    "                    if len(range(match[2][0], match[2][1] + 1)) == 1:\n",
    "                        file_text += token + before_tag + \"S-ARG1\" + \"\\n\"\n",
    "                    elif i == match[2][0]:\n",
    "                        file_text += token + before_tag + \"B-ARG1\" + \"\\n\"\n",
    "                    elif i > match[2][0] and i < match[2][1]:\n",
    "                        file_text += token + before_tag + \"I-ARG1\" + \"\\n\"\n",
    "                    elif i == match[2][1]:\n",
    "                        file_text += token + before_tag + \"E-ARG1\" + \"\\n\"\n",
    "                else:\n",
    "                    file_text += token + before_tag + \"O\" + \"\\n\"\n",
    "            file_text += \"\\n\"\n",
    "    with open(dir + \"/\" + name + \".txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            txt_f = f.read()\n",
    "        except:\n",
    "            txt_f = \"\"\n",
    "        txt_f += file_text\n",
    "        f.write(txt_f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:06:26.116806800Z",
     "start_time": "2023-10-04T00:06:26.086253200Z"
    }
   },
   "id": "b624afc5b4760857"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_dataset(): \n",
    "    dataset = get_dataset()\n",
    "    extractions = {}\n",
    "    i = 0\n",
    "    split = 0\n",
    "    while split<=1:\n",
    "        for triple in dataset[split]:\n",
    "            sent = triple.phrase\n",
    "            sent_counter = 0\n",
    "            for token in nlp(sent):\n",
    "                sent_counter += 1\n",
    "                \n",
    "            ext = triple.gold_extractions[0]\n",
    "            merged = ext.arg0 + \" \" + ext.rel + \" \" + ext.arg1\n",
    "            ext_counter = 0\n",
    "            for token in nlp(merged):\n",
    "                ext_counter += 1\n",
    "                \n",
    "            ext = {\"length\" : ext_counter,\n",
    "                   \"merged\": merged,\n",
    "                   \"splited\" : {\n",
    "                       \"arg0\" : ext.arg0,\n",
    "                       \"rel\" : ext.rel,\n",
    "                       \"arg1\" : ext.arg1\n",
    "                        }\n",
    "                   }\n",
    "            extractions[i] = {\"sent\" : {\"sent\" : sent,\n",
    "                                        \"length\" : sent_counter\n",
    "                                        },\n",
    "                              \"ext\" : ext}\n",
    "            i+=1\n",
    "        split+=1\n",
    "    return extractions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:06:28.197739200Z",
     "start_time": "2023-10-04T00:06:28.180024100Z"
    }
   },
   "id": "c211adf3f2a0cd5d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processando TA4: 100%|██████████| 102616/102616 [11:30<00:00, 148.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'sent': {'sent': 'A linha de a cintura foi colocada mais alta e as saias se tornaram mais longas .',\n  'length': 17},\n 'ext': {'length': 6,\n  'merged': 'as saias se tornaram mais longas',\n  'splited': {'arg0': 'as saias',\n   'rel': 'se tornaram',\n   'arg1': 'mais longas'}}}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset()\n",
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:33:21.749983900Z",
     "start_time": "2023-10-04T00:06:31.255305300Z"
    }
   },
   "id": "3a0b98248ce982e9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "102615"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:35:17.281920900Z",
     "start_time": "2023-10-04T00:35:17.263774200Z"
    }
   },
   "id": "2de1c7b47dd63e77"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "clean_dataset = {}\n",
    "trash = {}\n",
    "i=0\n",
    "j=0\n",
    "for data in dataset:\n",
    "    arg1 = dataset[data]['ext']['splited']['arg0']\n",
    "    arg1_doc = nlp(arg1)\n",
    "    for tk in arg1_doc:\n",
    "        if tk.pos_ in ['NOUN', 'PROPN']:\n",
    "            clean_dataset[i] = dataset[data]\n",
    "            i+=1\n",
    "            break\n",
    "        if tk.text == arg1_doc[-1].text:\n",
    "            trash[j] = dataset[data]\n",
    "            j+=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:40:47.478346600Z",
     "start_time": "2023-10-04T00:35:28.575142900Z"
    }
   },
   "id": "921816484fb16634"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving data: 100%|██████████| 97238/97238 [26:29<00:00, 61.17it/s]\n"
     ]
    }
   ],
   "source": [
    "save_data(clean_dataset, 'TA4_corpus')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T01:11:07.587172400Z",
     "start_time": "2023-10-04T00:44:37.683010300Z"
    }
   },
   "id": "a6af5fbd3ddf7d4a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "97238"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:43:07.469710400Z",
     "start_time": "2023-10-04T00:43:07.438210400Z"
    }
   },
   "id": "ef27ece3dadc91e7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "5458"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trash)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:42:58.971833400Z",
     "start_time": "2023-10-04T00:42:58.941465Z"
    }
   },
   "id": "a2fca43fb7abb039"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sent': {'sent': 'No entanto , a ação de o governo , que pegou a gerência de a Jaguar de surpresa , pode frustrar o negócio minoritário de a GM obrigando-os a lutar por toda a Jaguar .', 'length': 35}, 'ext': {'length': 13, 'merged': 'a ação de o governo pode frustrar o negócio minoritário de a GM', 'splited': {'arg0': 'a ação de o governo', 'rel': 'pode frustrar o negócio minoritário de', 'arg1': 'a GM'}}}\n"
     ]
    }
   ],
   "source": [
    "print(clean_dataset[10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T00:44:16.641759700Z",
     "start_time": "2023-10-04T00:44:16.627726700Z"
    }
   },
   "id": "d90ed5ca5a902b9e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "same_string_extractions = {}\n",
    "minimum_extractions = {}\n",
    "small_extractions = {}\n",
    "long_extractions = {}\n",
    "small_sentences = {}\n",
    "\n",
    "long_sentences = {}\n",
    "long_sentece_small_extractions = {}\n",
    "\n",
    "for key in dataset:\n",
    "    inst = dataset[key]\n",
    "    sent_len = inst[\"sent\"][\"length\"]\n",
    "    ext_len = inst[\"ext\"][\"length\"]\n",
    "    if sent_len-1 == ext_len or sent_len == ext_len:\n",
    "        same_string_extractions[key] = inst\n",
    "    elif ext_len > 8:\n",
    "        long_extractions[key] = inst\n",
    "    elif ext_len == 3:\n",
    "        minimum_extractions[key] = inst\n",
    "    elif ext_len > 3 and ext_len <=10:\n",
    "        small_extractions[key] = inst\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T03:26:57.825853400Z",
     "start_time": "2023-09-19T03:26:57.713643600Z"
    }
   },
   "id": "54ec3545a1ab79e2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extractions with same sentence length:  16463\n",
      "extractions with one tag on arg0, one on rel and one on arg1:  927\n",
      "small extractions, token counter <= 8:  38553\n",
      "long extractions, token counter > 8:  61959\n"
     ]
    }
   ],
   "source": [
    "print(\"extractions with same sentence length: \",len(same_string_extractions))\n",
    "print(\"extractions with one tag on arg0, one on rel and one on arg1: \", len(minimum_extractions))\n",
    "print(\"small extractions, token counter <= 8: \", len(small_extractions))\n",
    "print(\"long extractions, token counter > 8: \", len(long_extractions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T03:27:41.664699500Z",
     "start_time": "2023-09-19T03:27:41.617777900Z"
    }
   },
   "id": "bd0105aa622260f8"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving data: 100%|██████████| 38553/38553 [08:56<00:00, 71.83it/s]\n"
     ]
    }
   ],
   "source": [
    "save_data(small_extractions, 'small')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T03:36:41.492557900Z",
     "start_time": "2023-09-19T03:27:44.669374800Z"
    }
   },
   "id": "eff164fd22e913d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
